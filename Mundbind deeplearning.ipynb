{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all packages\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "     \n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torch import nn\n",
    "import neptune.new as neptune\n",
    "import random\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cefc261",
   "metadata": {},
   "source": [
    "## Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the APi key, from outside the git folder.\n",
    "file = open(\"../API-key.txt\", \"r\")\n",
    "key = file.readlines()\n",
    "\n",
    "run = neptune.init(\n",
    "    project=\"Facemask-project/Facemask\",\n",
    "    api_token=key[0])\n",
    "\n",
    "# Choose the device:\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "run['namespace/Run_name'] = \"Test_run\"\n",
    "run['sys/tags'].add(['size_125', \"flip\", \"new_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5a256",
   "metadata": {},
   "source": [
    "## Loade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fef967",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#Create the data loader. This data\n",
    "class DataLoader():\n",
    "    def __init__(self, img_dir, img_dir2, transform = None, target_transform=None, transforms = None,\n",
    "                 random_perspective = None, flip = False, blur = False, new_mask = False, bright = False):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_dir2 = img_dir2\n",
    "        self.transform = transform\n",
    "        self.transforms = transforms\n",
    "        self.fileNames = os.listdir(img_dir)\n",
    "        self.random_perspective = random_perspective\n",
    "        self.flip = flip\n",
    "        self.blur = blur\n",
    "        self.new_mask = new_mask\n",
    "        self.bright = bright\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.fileNames[idx].rsplit(\"t-mask-\", 1 )[1]\n",
    "        img_path = os.path.join(self.img_dir, self.fileNames[idx])\n",
    "        img_path2 = os.path.join(self.img_dir2, label)\n",
    "        image = read_image(img_path)\n",
    "        target_image = read_image(img_path2)\n",
    "        image = image.clone().detach().float()\n",
    "        target_image = target_image.clone().detach().float()\n",
    "        if self.new_mask:\n",
    "            if random.uniform(0,1) > 0.5:\n",
    "                image = new_mask(image, target_image)\n",
    "        if self.transform:\n",
    "            if random.uniform(0,1) > 0.9:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            target_image = self.transforms(target_image)\n",
    "        if self.bright:\n",
    "            if random.uniform(0,1) > 0.5:\n",
    "                adjust = random.uniform(0.3, 1.8)\n",
    "                image = torchvision.transforms.functional.adjust_brightness(image, adjust)\n",
    "                target_image = torchvision.transforms.functional.adjust_brightness(target_image, adjust)\n",
    "        if self.flip:\n",
    "            if random.uniform(0,1) > 0.5:\n",
    "                image = torchvision.transforms.functional.hflip(image)\n",
    "                target_image = torchvision.transforms.functional.hflip(target_image)\n",
    "        if self.random_perspective:\n",
    "            if random.uniform(0,1) > 0.5:\n",
    "                scale = random.uniform(0,0.4)\n",
    "                startpoints, endpoints = transforms.RandomPerspective().get_params(50,50, distortion_scale=scale)\n",
    "                image = torchvision.transforms.functional.perspective(image, startpoints, endpoints)\n",
    "                target_image  = torchvision.transforms.functional.perspective(target_image, startpoints, endpoints)\n",
    "        \n",
    "        return image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aed038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_mask(image, target):\n",
    "    #Get mask pixels\n",
    "    old_mask = torch.abs(image - target)\n",
    "    mask_index = np.where(old_mask != 0, True, False)\n",
    "    # New mask or random colours\n",
    "    if random.uniform(0,1) > 0.5:\n",
    "        image[0, mask_index[0]] = random.uniform(0,255)\n",
    "        image[1, mask_index[1]] = random.uniform(0,255)\n",
    "        image[2, mask_index[2]] = random.uniform(0,255)\n",
    "        return torch.tensor(image)\n",
    "    else:\n",
    "        mask_list = os.listdir(\"pictures_of_facemask\")\n",
    "        mask_number = random.randint(0, len(mask_list)-1)\n",
    "        mask_image = read_image(os.path.join(\"pictures_of_facemask\", mask_list[mask_number]))\n",
    "        resize = transforms.Resize((125,125))\n",
    "        mask_image = resize(mask_image)\n",
    "        \n",
    "        # Remove the opacity dimension\n",
    "        if mask_image.size()[0] == 4:\n",
    "            mask_image = mask_image[0:3]\n",
    "        new_image = np.where(old_mask != 0, mask_image, image)\n",
    "        return torch.tensor(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose batch_size:\n",
    "batch_size = 25\n",
    "\n",
    "#create transformer with mean = 0 and std = 1.\n",
    "transformer=transforms.Compose([ transforms.Normalize((0, 0, 0), (1, 1, 1))])\n",
    "\n",
    "#Create the dataset with the dataloader:\n",
    "traindataset = DataLoader(\"resized_images/125_with_mask_train\", \"resized_images/125_without_mask_train\",\n",
    "                     transforms = transformer,\n",
    "                    random_perspective = False,\n",
    "                       flip = True,\n",
    "                       new_mask = True,) \n",
    "\n",
    "# split the dataset into test and train set.\n",
    "testdataset = DataLoader(\"resized_images/125_with_mask_test\", \"resized_images/125_without_mask_test\",\n",
    "                     transforms = transformer,\n",
    "                    random_perspective = None) \n",
    "\n",
    "valdataset = DataLoader(\"resized_images/125_with_mask_val\", \"resized_images/125_without_mask_val\",\n",
    "                     transforms = transformer,\n",
    "                    random_perspective = None) \n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valdataset, \n",
    "                                          batch_size=5, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "# Set up dataloader and the test\n",
    "trainloader = torch.utils.data.DataLoader(traindataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testdataset, \n",
    "                                          batch_size=25, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True)\n",
    "dataiter = iter(trainloader)\n",
    "valiter= iter(valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e77b61",
   "metadata": {},
   "source": [
    "## Netværk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e09f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with auto encoder:\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "           \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(2700,2048)\n",
    "\n",
    "        \n",
    "\n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(8, 16, 5, padding=2) \n",
    "        self.conv2 = nn.Conv2d(16, 32, 9, padding=4)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        self.conv4 = nn.Conv2d(64, 32, 5, padding=2)\n",
    "        self.conv5 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(16, 12, 3, padding=1)\n",
    "       \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.unflatten = nn.Unflatten(dim =1, unflattened_size = (8,16,16))\n",
    "        \n",
    "        #Decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(8, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 32, 2, stride=2, padding = 1)\n",
    "        self.decode_conv1 =  nn.Conv2d(32, 64, 5, padding=2)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(64, 32 , 2, stride=2,)\n",
    "        self.t_conv4 = nn.ConvTranspose2d(32, 16, 2, stride=1)\n",
    "        self.decode_conv2 = nn.Conv2d(16, 8, 5, padding=2)\n",
    "        self.decode_conv3 = nn.Conv2d(8, 3, 3, padding=1)\n",
    "        \n",
    "\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv6(x))\n",
    "        return x\n",
    "    \n",
    "    def flatlayer(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.unflatten(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "\n",
    "        x = F.relu(self.decode_conv1(x))\n",
    "\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "\n",
    "        x = F.relu(self.t_conv4(x))\n",
    "\n",
    "        x = F.relu(self.decode_conv2(x))\n",
    "\n",
    "        x = F.relu(self.decode_conv3(x))\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.flatlayer(x)\n",
    "        x = self.decode(x)      \n",
    "        return x\n",
    "\n",
    "\n",
    "#Instantiate the model\n",
    "model = ConvAutoencoder()\n",
    "\n",
    "#Send the model to the device\n",
    "model.to(device)\n",
    "# check the model output is the correct size\n",
    "model(dataiter.next()[0].to(device)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3f474",
   "metadata": {},
   "source": [
    "## Træning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "#Loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "#Optimizer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
    "\n",
    "def update_params():\n",
    "    run[\"parameters\"] = {\"lr\": learning_rate,\n",
    "                         \"optim\": str(optimizer).split(\" \")[0],\n",
    "                         \"loss_function\" : \"MSE\",\n",
    "                         \"Batch_size\": batch_size,\n",
    "                         \"image_size\": 125\n",
    "                        }\n",
    "update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96562cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with a loss function with both MSE and SSIM\n",
    "def loss_function(output, target):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    mse = criterion(output, target)\n",
    "    mse = mse/1000\n",
    "    ssim_loss = 1 - ssim(output, target)\n",
    "    # see how they match up.\n",
    "    if random.randint(1,400) == 1:\n",
    "        print(mse, ssim_loss)\n",
    "    return mse + ssim_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ddf103",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(traindataset, \n",
    "                                          batch_size=5, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valloader = valloader):\n",
    "    data = valiter.next()\n",
    "    mask_image, image = data\n",
    "    mask_image , image = mask_image.to(device), image.to(device)\n",
    "    image = image.clone().detach().float()\n",
    "    mask_image = mask_image.clone().detach().float()\n",
    "    outputs = model(mask_image)\n",
    "    loss_ssim = ssim(outputs, image).item()\n",
    "    criterion2 = torch.nn.MSELoss()\n",
    "    loss_MSE = criterion2(outputs, image).item()\n",
    "    return loss_ssim, loss_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epochs\n",
    "def train(model, n_epochs = 10, criterion = criterion, optimizer = optimizer, trainloader = trainloader):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        #Training\n",
    "        for data in trainloader:\n",
    "            mask_image, image = data\n",
    "            mask_image , image = mask_image.to(device), image.to(device)\n",
    "            image = image.clone().detach().float()\n",
    "            mask_image = mask_image.clone().detach().float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mask_image)\n",
    "            loss = criterion(outputs, image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch % 1 == 0):\n",
    "            train_loss += loss.item()\n",
    "            run[\"train/loss\"].log(train_loss)\n",
    "            ssim_acc, mse_loss = validation(model = model)\n",
    "            run[\"train/val_ssim\"].log(ssim_acc)\n",
    "            run[\"train/val_mse\"].log(mse_loss)\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}   Validation accuracy: {}'.format(epoch, train_loss, ssim_acc))\n",
    "        if (epoch % 50 ==0):\n",
    "            checpoint_name = \"checkpoint\" + str(epoch)\n",
    "            save_checkpoint(model, checpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295a692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "train(model, n_epochs = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ab13b",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_image(image, model, correct_image, neptune_upload = False):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "    ax1.imshow(np.transpose(image.numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    ax2.imshow(np.transpose(correct_image.numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    image = torch.unsqueeze(image,0)\n",
    "    image = image.clone().detach().float()\n",
    "    image = image.to(device)\n",
    "    output = model(image)\n",
    "    output = torch.squeeze(output,0)\n",
    "    if device.type == 'cuda':\n",
    "        output = torch.Tensor.cpu(output).clone().detach()\n",
    "        correct = torch.Tensor.cpu(correct_image).clone().detach()\n",
    "    else:\n",
    "        output = output.clone().detach()\n",
    "        correct = correct_image.clone().detach()\n",
    "        \n",
    "\n",
    "    ax3.imshow(np.transpose(output.numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    error = torch.sum(torch.abs(output - correct), 0)\n",
    "    ax4.imshow(error.numpy().astype('uint8'))\n",
    "    if neptune_upload:\n",
    "        run['Test_figures'].upload(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7faa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = DataLoader(\"Data_small/125_with_mask_test\", \"Data_small/125_without_mask_test\",\n",
    "                     transform = transformer,\n",
    "                    random_perspective = None)\n",
    "testloader = torch.utils.data.DataLoader(testdataset, \n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=True)\n",
    "dataiter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee848a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test a Model\n",
    "model.eval()\n",
    "\n",
    "\n",
    "images, images2 = dataiter.next()\n",
    "#print_image(images[0], model = model2, correct_image = images2[0])\n",
    "print_image(images[0], model = model, correct_image = images2[0], neptune_upload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test data\n",
    "testiter = iter(testloader)\n",
    "tot_mse = 0\n",
    "tot_ssim = 0\n",
    "for i in range(20):\n",
    "    ssim1, mse = validation(model)\n",
    "    tot_mse += mse\n",
    "    tot_ssim += ssim1\n",
    "\n",
    "print(tot_mse/20)\n",
    "print(tot_ssim/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a5fa9",
   "metadata": {},
   "source": [
    "## Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model as a artifact\n",
    "def save_checkpoint(model, checkpoint_name):\n",
    "    checkpoint_name_ext = checkpoint_name + \".pt\"\n",
    "    checkpoints = os.listdir(\"Checkpoints\")\n",
    "    path = os.path.join(\"Checkpoints\", checkpoint_name_ext)\n",
    "    try_number = 0\n",
    "    while (os.path.exists(path)):\n",
    "        try_number += 1\n",
    "        checkpoint_name_ext = checkpoint_name + \"_\" + str(try_number) + \".pt\"\n",
    "        path = os.path.join(\"Checkpoints\", checkpoint_name_ext)\n",
    "    torch.save(model, path)\n",
    "    neptune_checkpoint_path = 'model_checkpoints/' + checkpoint_name + \"_\" + str(try_number)\n",
    "    run[neptune_checkpoint_path].upload(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(model,\"size_125_new_mask5_90_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08182496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Checkpoints/size_125_new_mask5_90_epochs_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run['sys/tags'].add([\"Good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08f312",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3befa",
   "metadata": {},
   "source": [
    "## Test på rigtige mennesker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e77559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create the data loader\n",
    "class DataLoader_human(Dataset):\n",
    "    def __init__(self, img_dir, transform = None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.fileNames = os.listdir(img_dir)\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.fileNames[idx])\n",
    "        #image = torchvision.io.ImageReadMode.RGB(img_path)\n",
    "        try:\n",
    "            image = read_image(img_path, mode = torchvision.io.image.ImageReadMode.RGB)\n",
    "        except:\n",
    "            print(\"An exception occurred\") \n",
    "        if image.size()[0] == 4:\n",
    "            image = image[0:3]\n",
    "        image = torch.tensor(image).float()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "def print_image_human(image, model):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(np.transpose(image.numpy(), (1, 2, 0)).astype('uint8'))\n",
    "    image = torch.unsqueeze(image,0)\n",
    "    image = torch.tensor(image).float()\n",
    "    image = image.to(device)\n",
    "    output = model(image)\n",
    "    print(ssim(image, output))\n",
    "    output = torch.squeeze(output,0)\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        output = torch.Tensor.cpu(output).detach().numpy()\n",
    "    else:\n",
    "        output = output.detach().numpy()\n",
    "        \n",
    "\n",
    "    ax2.imshow(np.transpose(output, (1, 2, 0)).astype('uint8'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841798eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "transformer=transforms.Compose([\n",
    "                               transforms.Normalize((0, 0, 0), (1, 1, 1)),\n",
    "                               transforms.Resize((125,125))\n",
    "                           ])\n",
    "\n",
    "dataset_human = DataLoader_human(\"Data_real_human\",transform = transformer) \n",
    "batch_size = 10\n",
    "humanloader = torch.utils.data.DataLoader(dataset_human, \n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0)\n",
    "\n",
    "dataiter_human = iter(humanloader)\n",
    "for i in range(7):\n",
    "    images = dataiter_human.next()\n",
    "    print_image_human(images[0], model = model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "neptune": {
   "notebookId": "206798db-397a-4951-862d-4ecfafdf6ab0",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
